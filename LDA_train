import re
import nltk
import pickle
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.corpora import Dictionary
from gensim.models import LdaModel
from sklearn.datasets import fetch_20newsgroups

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(text):
    """
    Cleans a document by removing non-alphabetic characters, tokenizing,
    converting to lowercase, removing stopwords, and lemmatizing.
    """
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    words = word_tokenize(text.lower())
    stop_words = set(stopwords.words('english'))
    return [word for word in words if word not in stop_words and len(word) > 2]

# Load dataset and preprocess
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
documents = newsgroups.data
processed_docs = [preprocess_text(doc) for doc in documents]

# Create dictionary and corpus
dictionary = Dictionary(processed_docs)
dictionary.filter_extremes(no_below=15, no_above=0.5)
corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

# Train LDA model
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=10, random_state=42)

# Save the model and dictionary
with open("lda_model.pkl", "wb") as f:
    pickle.dump(lda_model, f)
with open("lda_dictionary.pkl", "wb") as f:
    pickle.dump(dictionary, f)
with open("lda_corpus.pkl", "wb") as f:
    pickle.dump(corpus, f)

print("LDA model training completed and saved.")
