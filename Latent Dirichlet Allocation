"""
Category learning from corpora via Latent Dirichlet Allocation. 
This file is self-contained
"""

# Step 1: Load the dataset
from sklearn.datasets import fetch_20newsgroups

# Fetch the dataset and remove metadata (headers, footers, quotes)
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
documents = newsgroups.data  # List of raw text documents

# Step 2: Preprocess the text using NLTK
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize  # Tokenizes: "Hello, world!" -> ['hello', ',', 'world', '!']
from nltk.stem import WordNetLemmatizer  # Converts words to base form, e.g., verbs to infinitives

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    """
    Cleans a document by removing non-alphabetic characters, tokenizing, 
    converting to lowercase, removing stopwords, and lemmatizing.
    """
    # Remove non-alphabetic characters (numbers, punctuation, etc.)
    text = re.sub(r'[^a-zA-Z]', ' ', text)
    
    # Convert to lowercase and tokenize
    words = word_tokenize(text.lower())
    
    # Remove stopwords and lemmatize words
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    
    return [lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 2]

# Apply preprocessing to all documents
processed_docs = [preprocess_text(doc) for doc in documents]

# Step 3: Create Dictionary and Corpus
from gensim.corpora import Dictionary

# Convert processed documents into a dictionary
# Example: [["cat", "dog"], ["dog", "mouse", "cat"]] -> {0: "cat", 1: "dog", 2: "mouse"}
dictionary = Dictionary(processed_docs)

# Filter out extremely rare and very frequent words
dictionary.filter_extremes(no_below=15, no_above=0.5)

# Convert documents into Bag-of-Words (BoW) format
# Example: ["cat", "dog", "cat"] -> [(0, 2), (1, 1)] (where 0 is "cat", 1 is "dog")
corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

# Step 4: Train the LDA model
from gensim.models import LdaModel

# Train the LDA model with 20 topics
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=10, random_state=42)

# Step 5: Display the topics (printed by Hasan)
for idx, topic in lda_model.print_topics(num_topics=10, num_words=10):
    print(f"Topic {idx}: {topic}")
