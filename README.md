# topic_modelling_ETM
This is an actually amazing implementation of Embedded Topic Modeling for category learning. Every prior attempt‚Äîfrom Dieng, Ruiz, and Blei included‚Äîreads like a war crime against code clarity.

While prior implementations‚Äîincluding those by Dieng, Ruiz, and Blei‚Äîlaid the theoretical groundwork, their codebases often lacked clarity and Pythonic design. This repository addresses those shortcomings with clean, accessible, and well-structured implementations.

In addition to ETM, this repo also includes a Latent Dirichlet Allocation (LDA) model, as LDA is the natural predecessor to ETM. ETM improves upon LDA in nearly every respect‚Äîsemantic richness, generalization, and compatibility with modern deep learning tools. A comparison and analysis are included below.

LDA model: implemented by me
ETM model: implemented by me, Antony Z., and Hassan A.



## Embedded Topic Modeling (ETM) ‚Äì Intuition & Math

**Embedded Topic Modeling (ETM)** is a modern topic model that blends *word embeddings* with *probabilistic topics by placing **words and topics in the same vector space**.


### How It Works (Generative Model)

Let:
- \( V \): vocabulary size  
- \( K \): number of topics  
- \( d \): embedding dimension  
- \( \rho \in \mathbb{R}^{d \times V} \): word embeddings  
- \( \alpha \in \mathbb{R}^{K \times d} \): topic embeddings  
- \( \theta \in \Delta^{K-1} \): topic proportions for a document (sampled from a VAE)

Steps to generate a document:
1. Encode document into latent topic vector:  
   \[
   \theta \sim \text{VAE encoder}(x)
   \]
2. Compute topic mixture vector:
   \[
   z = \alpha^\top \theta \in \mathbb{R}^d
   \]
3. Probability of word \( w \):
   \[
   p(w \mid \theta) = \frac{\exp(\rho_w^\top z)}{\sum_{v=1}^V \exp(\rho_v^\top z)} = \text{softmax}(\rho^\top z)
   \]

So: **Words are more likely if they're close (in embedding space) to the topic mixture.**

### How It Infers
Unlike LDA, which uses Dirichlet priors, ETM uses a **Variational Autoencoder (VAE)** framework to learn:

- **\( \theta \)**: inferred via a neural encoder (e.g., MLP)  
- **\( \rho \)** and **\( \alpha \)**: learned through gradient descent

The model optimizes the **Evidence Lower Bound (ELBO)**:
\[
\text{ELBO} = \mathbb{E}_{q(\theta \mid x)} [ \log p(x \mid \theta) ] - \text{KL}(q(\theta \mid x) \parallel p(\theta))
\]
If done well, this balances:
- **Reconstruction**: matching predicted words to actual ones  
- **Regularization**: keeping inferred topic proportions close to the prior

---

### Why ETM?

- Learns *semantic* topics using embeddings  
- Generalizes better to rare or unseen words  
- Fully differentiable and compatible with modern DL tools  
- Builds a bridge between probabilistic modeling and representation learning

## Latent Dirichlet Allocation (LDA) ‚Äì Intuition & Math

Absolutely‚Äîlet‚Äôs improve the LDA section to better explain **how it learns**: what‚Äôs observed, what‚Äôs latent, and how the model infers topics from raw text. Here‚Äôs the updated and more intuitive version, still Markdown-ready for a GitHub README:

---

## Latent Dirichlet Allocation (LDA) ‚Äì Intuition & Math

**Latent Dirichlet Allocation (LDA)** is a classic probabilistic model that discovers hidden topics in documents. It assumes:

- Topics are distributions over words (e.g., ‚Äúastronomy‚Äù ‚Üí *planet*, *orbit*, *NASA*)  
- Documents are mixtures of those topics (e.g., 70% astronomy, 30% technology)  
- Words in a document are generated by first picking a topic, then picking a word from that topic

### As a Generative Model:

For each document \( d \), LDA assumes the following process:

1. Sample document's topic distribution:  
   \[
   \theta_d \sim \text{Dirichlet}(\alpha)
   \]
2. For each word position \( n \) in the document:
   - Sample a topic:  
     \[
     z_{dn} \sim \text{Categorical}(\theta_d)
     \]
   - Sample a word from that topic:  
     \[
     w_{dn} \sim \text{Categorical}(\phi_{z_{dn}})
     \]
Here:
- \( \phi_k \sim \text{Dirichlet}(\beta) \): each topic's word distribution  
- Only \( w_{dn} \) (words) are observed  
- \( z_{dn} \), \( \theta_d \), and \( \phi_k \) are latent and must be inferred


### üîÑ How It Infers

Exact inference is intractable, we use **approximate inference** like. My version uses variational inference. These methods iteratively update guesses for the topic distributions until they match the observed word patterns well.

LDA learns:
- **Topics**: \( \phi_k \), distributions over words  
- **Document-topic mixtures**: \( \theta_d \)  
- **Topic assignments per word**: \( z_{dn} \)

### Alternatives
See below and decide depending on what aspect of category learnings you want to get right.
https://paperswithcode.com/dataset/20-newsgroups


### Alternatives



